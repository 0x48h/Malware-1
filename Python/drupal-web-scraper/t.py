from Queue import Queue
from threading import Thread
import urllib2, time, re, socket

threadcount = 30
domaincount = 200
timeout = 5
serverurl = "http://74.118.139.203:7575/?cmd="
domain_queue = Queue()

def senddomains(domains):
	count = 0
	while count < 10:
		count = count + 1
		req = urllib2.Request(serverurl + "adddomains", domains)
		try:
			urllib2.urlopen(req)
		except Exception:
			print "Error sending domains, waiting"
			time.sleep(6)
			continue
		return

def senddrupal(domainid):
	count = 0
	while count < 10:
		count = count + 1
		try:
			urllib2.urlopen(serverurl + "drupal&did=" + domainid).read()
		except Exception:
			print "Error sending drupal, waiting"
			time.sleep(6)
			continue
		return

def scrapedomain(i, q):
	while not q.empty():
		ld_q = Queue()
		drupal = False
		url = q.get()
		url = url.split("::")
		if url[1][-1:] == "/":
			url[1] = url[1][:-1]
		print "domain started " + url[1]
		html = ""
		try:
			socket.setdefaulttimeout(timeout)
			html = urllib2.urlopen(url[1]).read()
		except Exception: 
			q.task_done()
			continue
		if "Drupal" in html:
			drupal = True
		domlinks = re.findall('<a .*?href="(http.*?)"', html)
		for l in domlinks:
			try:
				l=l.split("/")[0] + "//" + l.split("/")[2].split("?")[0]
				ld_q.put(l)
			except Exception: 
				continue
		reglinks = re.findall('<a .*?href="(/.*?)"', html)
		count = 0
		for l in reglinks:
			count = count + 1
			if count == 10:
				break
			html2 = ""
			try:
				socket.setdefaulttimeout(timeout)
				html2 = urllib2.urlopen(url[1] + l).read()
			except Exception: 
				continue
			if "Drupal" in html2:
				drupal = True
			domlinks2 = re.findall('<a .*?href="(http.*?)"', html2)
			for l in domlinks2:
				try:
					l=l.split("/")[0] + "//" + l.split("/")[2].split("?")[0]
					ld_q.put(l)
				except Exception: 
					continue
		if drupal == True:
			senddrupal(url[0])
		dlist = ""
		while ld_q.qsize() > 0:
			dom = ld_q.get()
			if dom not in dlist:
				dlist = dlist + dom + "\r\n"
		senddomains(dlist)
		q.task_done()


while 1:
	data = ""
	while data == "":
		try:
			data=urllib2.urlopen(serverurl + "getworkpy&count=" + str(domaincount)).read()
		except Exception:
			print "Error grabbing urllist, sleeping"
			time.sleep(2)
			pass
	domains = data.splitlines()
	map(domain_queue.put, domains)
	for i in range(threadcount):
		worker = Thread(target=scrapedomain, args=(i, domain_queue,))
		worker.setDaemon(True)
		worker.start()
	domain_queue.join()
